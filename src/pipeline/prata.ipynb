{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import logging\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "CATALOG = \"\"\n",
    "\n",
    "VOLUME_CATALOG = \"main\"\n",
    "VOLUME_SCHEMA = \"engenharia_dados\"\n",
    "VOLUME_NAME = \"aviacao_landing\"\n",
    "\n",
    "BRONZE_SCHEMA = \"aviacao_bronze\"\n",
    "SILVER_SCHEMA = \"aviacao_silver\"\n",
    "META_SCHEMA = \"aviacao_meta\"\n",
    "ORIGEM_SISTEMA = \"postgres-aviacao\"\n",
    "\n",
    "TABLE_CONFIGS: Dict[str, Dict] = {\n",
    "    \"companhias_aereas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"modelos_avioes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeroportos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeronaves\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"funcionarios\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"clientes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"voos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"reservas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bilhetes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bagagens\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"manutencoes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"tripulacao_voo\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "}\n",
    "\n",
    "TABLE_SCHEMAS: Dict[str, StructType] = {}\n",
    "\n",
    "logger = logging.getLogger(\"aviacao_silver\")\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s [%(levelname)s] %(name)s - %(message)s\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def qname(schema: str, table: str) -> str:\n",
    "    if CATALOG:\n",
    "        return f\"{CATALOG}.{schema}.{table}\"\n",
    "    return f\"{schema}.{table}\"\n",
    "\n",
    "\n",
    "def now_utc():\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "\n",
    "def init_schema(schema_name: str) -> None:\n",
    "    schema_qualified = f\"{CATALOG}.{schema_name}\" if CATALOG else schema_name\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_qualified}\")\n",
    "\n",
    "\n",
    "def silver_dim_table_name(table_name: str) -> str:\n",
    "    return qname(SILVER_SCHEMA, f\"dim_{table_name}\")\n",
    "\n",
    "\n",
    "def meta_silver_watermark_table() -> str:\n",
    "    return qname(META_SCHEMA, \"silver_bronze_watermark\")\n",
    "\n",
    "def init_meta_silver_watermark() -> None:\n",
    "    init_schema(META_SCHEMA)\n",
    "    meta_table = meta_silver_watermark_table()\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {meta_table} (\n",
    "            tabela STRING NOT NULL,\n",
    "            ultima_data_ref TIMESTAMP NOT NULL,\n",
    "            ultima_execucao_ts TIMESTAMP NOT NULL\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            ALTER TABLE {meta_table}\n",
    "            ADD CONSTRAINT pk_silver_bronze_watermark PRIMARY KEY (tabela)\n",
    "        \"\"\")\n",
    "    except Exception:\n",
    "        logger.info(\n",
    "            f\"[META] Constraint pk_silver_bronze_watermark já existe em {meta_table}, \"\n",
    "            f\"ignorando criação.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_last_processed_ts(table_name: str) -> Optional[datetime]:\n",
    "    meta_table = meta_silver_watermark_table()\n",
    "    if not spark.catalog.tableExists(meta_table):\n",
    "        return None\n",
    "\n",
    "    df = spark.table(meta_table).filter(F.col(\"tabela\") == table_name)\n",
    "    if df.limit(1).count() == 0:\n",
    "        return None\n",
    "\n",
    "    row = df.agg(F.max(\"ultima_data_ref\").alias(\"max_ts\")).collect()[0]\n",
    "    return row[\"max_ts\"]\n",
    "\n",
    "\n",
    "def update_watermark(table_name: str, last_bronze_ts: datetime) -> None:\n",
    "    meta_table = meta_silver_watermark_table()\n",
    "    ts_str = last_bronze_ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {meta_table} AS tgt\n",
    "        USING (\n",
    "            SELECT\n",
    "                '{table_name}' AS tabela,\n",
    "                TIMESTAMP '{ts_str}' AS ultima_data_ref,\n",
    "                current_timestamp() AS ultima_execucao_ts\n",
    "        ) AS src\n",
    "        ON tgt.tabela = src.tabela\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            tgt.ultima_data_ref    = src.ultima_data_ref,\n",
    "            tgt.ultima_execucao_ts = src.ultima_execucao_ts\n",
    "        WHEN NOT MATCHED THEN INSERT (tabela, ultima_data_ref, ultima_execucao_ts)\n",
    "        VALUES (src.tabela, src.ultima_data_ref, src.ultima_execucao_ts)\n",
    "    \"\"\")\n",
    "\n",
    "    logger.info(f\"[{table_name}] Watermark atualizado para {ts_str}.\")\n",
    "\n",
    "def bronze_changelog_table_name(table_name: str) -> str:\n",
    "    return qname(BRONZE_SCHEMA, f\"{table_name}_changelog\")\n",
    "\n",
    "\n",
    "def read_bronze_incremental(table_name: str, last_processed_ts: Optional[datetime]) -> DataFrame:\n",
    "    bronze_table = bronze_changelog_table_name(table_name)\n",
    "\n",
    "    if not spark.catalog.tableExists(bronze_table):\n",
    "        raise ValueError(f\"A tabela Bronze {bronze_table} não existe.\")\n",
    "\n",
    "    df = spark.table(bronze_table)\n",
    "\n",
    "    if last_processed_ts is not None:\n",
    "        df = df.filter(F.col(\"bronze_load_ts\") > F.lit(last_processed_ts))\n",
    "\n",
    "    return df\n",
    "\n",
    "def ensure_silver_dim_table(table_name: str, df_sample: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Cria dim_<tabela> copiando o schema da Bronze + colunas SCD2:\n",
    "      vigencia_inicio, vigencia_fim, is_current, aud_dh_criacao, aud_dh_alteracao, attr_hash\n",
    "    \"\"\"\n",
    "    init_schema(SILVER_SCHEMA)\n",
    "    silver_table = silver_dim_table_name(table_name)\n",
    "\n",
    "    if spark.catalog.tableExists(silver_table):\n",
    "        logger.info(f\"[{table_name}] Dimensão Silver {silver_table} já existe.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"[{table_name}] Criando dimensão Silver {silver_table} baseada no schema da Bronze.\")\n",
    "\n",
    "    base_schema: StructType = df_sample.schema\n",
    "    existing_cols = {f.name for f in base_schema.fields}\n",
    "\n",
    "    metadata_fields: List[StructField] = []\n",
    "\n",
    "    if \"vigencia_inicio\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"vigencia_inicio\", TimestampType(), nullable=True)\n",
    "        )\n",
    "    if \"vigencia_fim\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"vigencia_fim\", TimestampType(), nullable=True)\n",
    "        )\n",
    "    if \"is_current\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"is_current\", BooleanType(), nullable=True)\n",
    "        )\n",
    "    if \"aud_dh_criacao\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"aud_dh_criacao\", TimestampType(), nullable=True)\n",
    "        )\n",
    "    if \"aud_dh_alteracao\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"aud_dh_alteracao\", TimestampType(), nullable=True)\n",
    "        )\n",
    "    if \"attr_hash\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"attr_hash\", StringType(), nullable=True)\n",
    "        )\n",
    "\n",
    "    silver_schema = StructType(list(base_schema.fields) + metadata_fields)\n",
    "    empty_df = spark.createDataFrame([], silver_schema)\n",
    "\n",
    "    (\n",
    "        empty_df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(silver_table)\n",
    "    )\n",
    "\n",
    "    logger.info(f\"[{table_name}] Dimensão Silver {silver_table} criada como SCD2.\")\n",
    "\n",
    "def process_table(table_name: str) -> None:\n",
    "    if table_name not in TABLE_CONFIGS:\n",
    "        raise ValueError(f\"Tabela '{table_name}' não está configurada em TABLE_CONFIGS.\")\n",
    "\n",
    "    business_key_cols = TABLE_CONFIGS[table_name][\"business_key\"]\n",
    "\n",
    "    logger.info(f\"================ INÍCIO SILVER (SCD2): {table_name} ================\")\n",
    "\n",
    "    last_ts = get_last_processed_ts(table_name)\n",
    "    logger.info(f\"[{table_name}] Último bronze_load_ts processado: {last_ts}\")\n",
    "\n",
    "    df_bronze = read_bronze_incremental(table_name, last_ts)\n",
    "\n",
    "    if df_bronze.limit(1).count() == 0:\n",
    "        logger.info(f\"[{table_name}] Nenhum dado incremental na Bronze para processar.\")\n",
    "        logger.info(f\"================ FIM SILVER (sem dados): {table_name} ================\")\n",
    "        return\n",
    "\n",
    "    ensure_silver_dim_table(table_name, df_bronze)\n",
    "    silver_table = silver_dim_table_name(table_name)\n",
    "\n",
    "    stats = df_bronze.agg(\n",
    "        F.min(\"bronze_load_ts\").alias(\"min_bronze_ts\"),\n",
    "        F.max(\"bronze_load_ts\").alias(\"max_bronze_ts\"),\n",
    "    ).collect()[0]\n",
    "    max_bronze_ts = stats[\"max_bronze_ts\"]\n",
    "    logger.info(\n",
    "        f\"[{table_name}] Faixa incremental Bronze - bronze_load_ts: \"\n",
    "        f\"[{stats['min_bronze_ts']}, {stats['max_bronze_ts']}]\"\n",
    "    )\n",
    "\n",
    "    w = Window.partitionBy(*[F.col(c) for c in business_key_cols]).orderBy(\n",
    "        F.col(\"data_ref\").desc(),\n",
    "        F.col(\"bronze_load_ts\").desc(),\n",
    "    )\n",
    "\n",
    "    df_changes = (\n",
    "        df_bronze\n",
    "        .withColumn(\"row_number\", F.row_number().over(w))\n",
    "        .filter(F.col(\"row_number\") == 1)\n",
    "        .drop(\"row_number\")\n",
    "    )\n",
    "\n",
    "    technical_cols = set(business_key_cols + [\"data_ref\", \"bronze_load_ts\", \"origem_sistema\", \"change_op\"])\n",
    "    attr_cols = [c for c in df_changes.columns if c not in technical_cols]\n",
    "\n",
    "    df_changes = df_changes.withColumn(\n",
    "        \"attr_hash\",\n",
    "        F.sha2(F.concat_ws(\"||\", *[F.col(c).cast(\"string\") for c in attr_cols]), 256)\n",
    "    )\n",
    "\n",
    "    logger.info(f\"[{table_name}] Registros em df_changes: {df_changes.count()}\")\n",
    "\n",
    "    df_silver_current = spark.table(silver_table).filter(F.col(\"is_current\") == True)\n",
    "\n",
    "    join_expr = [\n",
    "        F.col(f\"chg.{c}\") == F.col(f\"dim.{c}\")\n",
    "        for c in business_key_cols\n",
    "    ]\n",
    "\n",
    "    df_join = df_changes.alias(\"chg\").join(\n",
    "        df_silver_current.alias(\"dim\"),\n",
    "        on=join_expr,\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    delta_dim = DeltaTable.forName(spark, silver_table)\n",
    "\n",
    "    df_to_close = (\n",
    "        df_join\n",
    "        .filter(\n",
    "            F.col(\"dim.attr_hash\").isNotNull() &\n",
    "            (F.col(\"dim.attr_hash\") != F.col(\"chg.attr_hash\"))\n",
    "        )\n",
    "        .select(\n",
    "            *[F.col(f\"chg.{c}\").alias(c) for c in business_key_cols],\n",
    "            F.col(\"chg.data_ref\").alias(\"data_ref\"),\n",
    "        )\n",
    "        .dropDuplicates(business_key_cols)\n",
    "    )\n",
    "\n",
    "    if df_to_close.limit(1).count() == 0:\n",
    "        logger.info(f\"[{table_name}] Nenhum registro atual na Silver para fechar (sem mudanças de atributo).\")\n",
    "    else:\n",
    "        qtd_close = df_to_close.count()\n",
    "        logger.info(f\"[{table_name}] Fechando {qtd_close} registros atuais na Silver.\")\n",
    "\n",
    "        cond_parts = [f\"dim.{c} = chg.{c}\" for c in business_key_cols]\n",
    "        cond_parts.append(\"dim.is_current = true\")\n",
    "        merge_condition_close = \" AND \".join(cond_parts)\n",
    "\n",
    "        (\n",
    "            delta_dim.alias(\"dim\")\n",
    "            .merge(\n",
    "                df_to_close.alias(\"chg\"),\n",
    "                merge_condition_close,\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                set={\n",
    "                    \"vigencia_fim\": \"chg.data_ref - INTERVAL 1 MICROSECOND\",\n",
    "                    \"is_current\": \"false\",\n",
    "                    \"aud_dh_alteracao\": \"current_timestamp()\",\n",
    "                }\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    df_to_insert = (\n",
    "        df_join\n",
    "        .filter(\n",
    "            F.col(\"dim.attr_hash\").isNull() |\n",
    "            (F.col(\"dim.attr_hash\") != F.col(\"chg.attr_hash\"))\n",
    "        )\n",
    "        .select(\"chg.*\")\n",
    "    )\n",
    "\n",
    "    if df_to_insert.limit(1).count() == 0:\n",
    "        logger.info(f\"[{table_name}] Nenhum registro novo/alterado para inserir na Silver.\")\n",
    "    else:\n",
    "        qtd_insert = df_to_insert.count()\n",
    "        logger.info(f\"[{table_name}] Inserindo {qtd_insert} registros (novos/alterados) na Silver.\")\n",
    "\n",
    "        dim_schema = spark.table(silver_table).schema\n",
    "        meta_cols = {\"vigencia_inicio\", \"vigencia_fim\", \"is_current\", \"aud_dh_criacao\", \"aud_dh_alteracao\", \"attr_hash\"}\n",
    "        base_cols = [f.name for f in dim_schema.fields if f.name not in meta_cols]\n",
    "\n",
    "        cond_parts = [f\"dim.{c} = chg.{c}\" for c in business_key_cols]\n",
    "        cond_parts.append(\"dim.is_current = true\")\n",
    "        merge_condition_insert = \" AND \".join(cond_parts)\n",
    "\n",
    "        values_map = {col: f\"chg.{col}\" for col in base_cols if col in df_to_insert.columns}\n",
    "\n",
    "        values_map.update({\n",
    "            \"vigencia_inicio\": \"chg.data_ref\",\n",
    "            \"vigencia_fim\": \"TIMESTAMP '9999-12-31 23:59:59'\",\n",
    "            \"is_current\": \"true\",\n",
    "            \"aud_dh_criacao\": \"current_timestamp()\",\n",
    "            \"aud_dh_alteracao\": \"current_timestamp()\",\n",
    "            \"attr_hash\": \"chg.attr_hash\",\n",
    "        })\n",
    "\n",
    "        (\n",
    "            delta_dim.alias(\"dim\")\n",
    "            .merge(\n",
    "                df_to_insert.alias(\"chg\"),\n",
    "                merge_condition_insert,\n",
    "            )\n",
    "            .whenNotMatchedInsert(\n",
    "                values=values_map\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    update_watermark(table_name, max_bronze_ts)\n",
    "\n",
    "    logger.info(f\"================ FIM SILVER (SCD2): {table_name} ================\")\n",
    "\n",
    "\n",
    "def main(tables: Optional[List[str]] = None) -> None:\n",
    "    logger.info(\"Iniciando o processamento da camada Silver (SCD2)...\")\n",
    "\n",
    "    init_schema(SILVER_SCHEMA)\n",
    "    init_meta_silver_watermark()\n",
    "\n",
    "    if tables is None:\n",
    "        tables = list(TABLE_CONFIGS.keys())\n",
    "\n",
    "    for tbl in tables:\n",
    "        process_table(tbl)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
