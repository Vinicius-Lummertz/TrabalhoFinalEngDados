{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import logging\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType\n",
    "\n",
    "CATALOG = \"\"\n",
    "\n",
    "VOLUME_CATALOG = \"main\"\n",
    "VOLUME_SCHEMA = \"engenharia_dados\"\n",
    "VOLUME_NAME = \"aviacao_landing\"\n",
    "\n",
    "LANDING_CSV_BASE_PATH = f\"/Volumes/{VOLUME_CATALOG}/{VOLUME_SCHEMA}/{VOLUME_NAME}/aviacao/landing\"\n",
    "\n",
    "BRONZE_SCHEMA = \"aviacao_bronze\"\n",
    "META_SCHEMA = \"aviacao_meta\"\n",
    "ORIGEM_SISTEMA = \"postgres-aviacao\"\n",
    "\n",
    "TABLE_CONFIGS: Dict[str, Dict] = {\n",
    "    \"companhias_aereas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"modelos_avioes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeroportos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeronaves\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"funcionarios\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"clientes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"voos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"reservas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bilhetes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bagagens\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"manutencoes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"tripulacao_voo\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "}\n",
    "\n",
    "TABLE_SCHEMAS: Dict[str, StructType] = {}\n",
    "\n",
    "logger = logging.getLogger(\"aviacao_bronze\")\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s [%(levelname)s] %(name)s - %(message)s\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def qname(schema: str, table: str) -> str:\n",
    "    if CATALOG:\n",
    "        return f\"{CATALOG}.{schema}.{table}\"\n",
    "    return f\"{schema}.{table}\"\n",
    "\n",
    "\n",
    "def now_utc():\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "\n",
    "def init_schema(schema_name: str) -> None:\n",
    "    schema_qualified = f\"{CATALOG}.{schema_name}\" if CATALOG else schema_name\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_qualified}\")\n",
    "\n",
    "\n",
    "def bronze_table_name(table_name: str) -> str:\n",
    "    return qname(BRONZE_SCHEMA, f\"{table_name}_changelog\")\n",
    "\n",
    "\n",
    "def meta_bronze_batches_table() -> str:\n",
    "    return qname(META_SCHEMA, \"bronze_landing_batches\")\n",
    "\n",
    "\n",
    "def get_landing_schema(table_name: str) -> Optional[StructType]:\n",
    "    return TABLE_SCHEMAS.get(table_name)\n",
    "\n",
    "\n",
    "def path_exists(path: str) -> bool:\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return len(files) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def init_meta_bronze_batches() -> None:\n",
    "    init_schema(META_SCHEMA)\n",
    "    meta_table = meta_bronze_batches_table()\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {meta_table} (\n",
    "            tabela STRING NOT NULL,\n",
    "            landing_batch_id STRING NOT NULL,\n",
    "            landing_path STRING NOT NULL,\n",
    "            first_seen_ts TIMESTAMP NOT NULL,\n",
    "            processed_ts TIMESTAMP NOT NULL\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def list_landing_batches(table_name: str) -> List[Tuple[str, str]]:\n",
    "    base_path = f\"{LANDING_CSV_BASE_PATH}/{table_name}\"\n",
    "    if not path_exists(base_path):\n",
    "        logger.info(f\"[{table_name}] Nenhum diretório de landing encontrado em {base_path}.\")\n",
    "        return []\n",
    "\n",
    "    batches: List[Tuple[str, str]] = []\n",
    "    for f in dbutils.fs.ls(base_path):\n",
    "        path = f.path\n",
    "        name = path.rstrip(\"/\").split(\"/\")[-1]\n",
    "        if name.startswith(\"batch_id=\"):\n",
    "            batch_id = name.split(\"batch_id=\")[-1]\n",
    "            batches.append((batch_id, path))\n",
    "    logger.info(f\"[{table_name}] Batches encontrados na landing: {[b[0] for b in batches]}\")\n",
    "    return batches\n",
    "\n",
    "\n",
    "def list_processed_batches(table_name: str) -> List[str]:\n",
    "    meta_table = meta_bronze_batches_table()\n",
    "    if not spark.catalog.tableExists(meta_table):\n",
    "        return []\n",
    "    df = spark.table(meta_table).filter(F.col(\"tabela\") == table_name)\n",
    "    rows = [r[\"landing_batch_id\"] for r in df.select(\"landing_batch_id\").distinct().collect()]\n",
    "    logger.info(f\"[{table_name}] Batches já processados na Bronze: {rows}\")\n",
    "    return rows\n",
    "\n",
    "\n",
    "def read_landing_incremental(table_name: str) -> Tuple[Optional[DataFrame], int, List[Tuple[str, str]]]:\n",
    "    landing_batches = list_landing_batches(table_name)\n",
    "    if not landing_batches:\n",
    "        return None, 0, []\n",
    "\n",
    "    processed_ids = set(list_processed_batches(table_name))\n",
    "    new_batches = [(bid, path) for (bid, path) in landing_batches if bid not in processed_ids]\n",
    "\n",
    "    if not new_batches:\n",
    "        logger.info(f\"[{table_name}] Nenhum batch_id novo na landing. Nada a processar.\")\n",
    "        return None, 0, []\n",
    "\n",
    "    logger.info(f\"[{table_name}] Batches novos a serem processados: {[b[0] for b in new_batches]}\")\n",
    "\n",
    "    reader = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"delimiter\", \";\")\n",
    "    )\n",
    "\n",
    "    schema = get_landing_schema(table_name)\n",
    "    if schema is not None:\n",
    "        reader = reader.schema(schema)\n",
    "    else:\n",
    "        logger.warning(\n",
    "            f\"[{table_name}] Usando inferSchema na landing. \"\n",
    "            f\"Configure TABLE_SCHEMAS para produção.\"\n",
    "        )\n",
    "        reader = reader.option(\"inferSchema\", \"true\")\n",
    "\n",
    "    df_all: Optional[DataFrame] = None\n",
    "    total_raw = 0\n",
    "\n",
    "    for batch_id, path in new_batches:\n",
    "        df_part = reader.csv(path)\n",
    "        df_part = df_part.withColumn(\"data_ref\", F.col(\"data_ref\").cast(\"timestamp\"))\n",
    "        if \"landing_load_ts\" in df_part.columns:\n",
    "            df_part = df_part.withColumn(\"landing_load_ts\", F.col(\"landing_load_ts\").cast(\"timestamp\"))\n",
    "        else:\n",
    "            logger.warning(f\"[{table_name}] Coluna landing_load_ts não encontrada no batch {batch_id}.\")\n",
    "\n",
    "        count_part = df_part.count()\n",
    "        total_raw += count_part\n",
    "        logger.info(f\"[{table_name}] Registros brutos no batch {batch_id}: {count_part}\")\n",
    "\n",
    "        if df_all is None:\n",
    "            df_all = df_part\n",
    "        else:\n",
    "            df_all = df_all.unionByName(df_part)\n",
    "\n",
    "    if df_all is None:\n",
    "        return None, 0, []\n",
    "\n",
    "    df_valid = df_all.filter(F.col(\"data_ref\").isNotNull())\n",
    "    total_valid = df_valid.count()\n",
    "    null_count = total_raw - total_valid\n",
    "    if null_count > 0:\n",
    "        logger.warning(\n",
    "            f\"[{table_name}] {null_count} registros descartados por data_ref nula.\"\n",
    "        )\n",
    "\n",
    "    if total_valid > 0:\n",
    "        stats = df_valid.agg(\n",
    "            F.min(\"data_ref\").alias(\"min_data_ref\"),\n",
    "            F.max(\"data_ref\").alias(\"max_data_ref\"),\n",
    "            F.min(\"landing_load_ts\").alias(\"min_landing_ts\"),\n",
    "            F.max(\"landing_load_ts\").alias(\"max_landing_ts\"),\n",
    "        ).collect()[0]\n",
    "        logger.info(\n",
    "            f\"[{table_name}] Faixa nos batches novos - \"\n",
    "            f\"data_ref: [{stats['min_data_ref']}, {stats['max_data_ref']}], \"\n",
    "            f\"landing_load_ts: [{stats['min_landing_ts']}, {stats['max_landing_ts']}]\"\n",
    "        )\n",
    "\n",
    "    if \"change_op\" in df_valid.columns:\n",
    "        dist = df_valid.groupBy(\"change_op\").count().collect()\n",
    "        for row in dist:\n",
    "            logger.info(\n",
    "                f\"[{table_name}] (landing filtrada) change_op={row['change_op']} \"\n",
    "                f\"-> {row['count']} registros\"\n",
    "            )\n",
    "\n",
    "    business_key_cols = TABLE_CONFIGS[table_name][\"business_key\"]\n",
    "    df_valid = df_valid.dropDuplicates(business_key_cols + [\"data_ref\"])\n",
    "    total_final = df_valid.count()\n",
    "    logger.info(\n",
    "        f\"[{table_name}] Registros após deduplicação (batches novos): {total_final}\"\n",
    "    )\n",
    "\n",
    "    if total_final == 0:\n",
    "        return None, 0, []\n",
    "\n",
    "    return df_valid, total_final, new_batches\n",
    "\n",
    "\n",
    "def ensure_bronze_changelog_table(table_name: str, df_sample: DataFrame) -> None:\n",
    "    bronze_table = bronze_table_name(table_name)\n",
    "\n",
    "    if spark.catalog.tableExists(bronze_table):\n",
    "        return\n",
    "\n",
    "    logger.info(f\"[{table_name}] Criando tabela Bronze change-log vazia: {bronze_table}\")\n",
    "\n",
    "    base_schema: StructType = df_sample.schema\n",
    "    existing_cols = {f.name for f in base_schema.fields}\n",
    "\n",
    "    metadata_fields = []\n",
    "\n",
    "    if \"bronze_load_ts\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"bronze_load_ts\", TimestampType(), nullable=False)\n",
    "        )\n",
    "\n",
    "    if \"bronze_batch_id\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"bronze_batch_id\", StringType(), nullable=False)\n",
    "        )\n",
    "\n",
    "    if \"origem_sistema\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"origem_sistema\", StringType(), nullable=False)\n",
    "        )\n",
    "\n",
    "    bronze_schema = StructType(list(base_schema.fields) + metadata_fields)\n",
    "\n",
    "    empty_df = spark.createDataFrame([], bronze_schema)\n",
    "\n",
    "    (\n",
    "        empty_df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(bronze_table)\n",
    "    )\n",
    "\n",
    "    logger.info(f\"[{table_name}] Tabela Bronze criada como append-only change-log.\")\n",
    "\n",
    "\n",
    "def register_processed_batches(table_name: str, batches: List[Tuple[str, str]]) -> None:\n",
    "    if not batches:\n",
    "        return\n",
    "    meta_table = meta_bronze_batches_table()\n",
    "    now_ts = now_utc()\n",
    "    rows = [\n",
    "        (table_name, batch_id, path, now_ts, now_ts)\n",
    "        for (batch_id, path) in batches\n",
    "    ]\n",
    "    df_meta = spark.createDataFrame(\n",
    "        rows,\n",
    "        [\"tabela\", \"landing_batch_id\", \"landing_path\", \"first_seen_ts\", \"processed_ts\"]\n",
    "    )\n",
    "    (\n",
    "        df_meta.write\n",
    "        .mode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(meta_table)\n",
    "    )\n",
    "    logger.info(f\"[{table_name}] Batches registrados como processados: {[b[0] for b in batches]}\")\n",
    "\n",
    "\n",
    "def merge_into_bronze_changelog(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    business_key_cols: List[str],\n",
    "    batch_id: str,\n",
    ") -> int:\n",
    "    bronze_table = bronze_table_name(table_name)\n",
    "\n",
    "    ensure_bronze_changelog_table(table_name, df)\n",
    "\n",
    "    df_enriched = (\n",
    "        df\n",
    "        .withColumn(\"bronze_load_ts\", F.lit(now_utc()))\n",
    "        .withColumn(\"bronze_batch_id\", F.lit(batch_id))\n",
    "        .withColumn(\"origem_sistema\", F.lit(ORIGEM_SISTEMA).cast(\"string\"))\n",
    "    )\n",
    "\n",
    "    dedup_cols = business_key_cols + [\"data_ref\"]\n",
    "    df_enriched = df_enriched.dropDuplicates(dedup_cols)\n",
    "\n",
    "    total_to_merge = df_enriched.count()\n",
    "    logger.info(f\"[{table_name}] Registros a serem mesclados no Bronze: {total_to_merge}\")\n",
    "\n",
    "    if total_to_merge == 0:\n",
    "        return 0\n",
    "\n",
    "    if \"change_op\" in df_enriched.columns:\n",
    "        dist = df_enriched.groupBy(\"change_op\").count().collect()\n",
    "        for row in dist:\n",
    "            logger.info(\n",
    "                f\"[{table_name}] (bronze) change_op={row['change_op']} \"\n",
    "                f\"-> {row['count']} registros\"\n",
    "            )\n",
    "\n",
    "    cond_parts = [f\"tgt.{col} = src.{col}\" for col in business_key_cols]\n",
    "    cond_parts.append(\"tgt.data_ref = src.data_ref\")\n",
    "    merge_condition = \" AND \".join(cond_parts)\n",
    "\n",
    "    delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "\n",
    "    (\n",
    "        delta_tbl.alias(\"tgt\")\n",
    "        .merge(\n",
    "            df_enriched.alias(\"src\"),\n",
    "            merge_condition\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    logger.info(f\"[{table_name}] MERGE em Bronze change-log concluído.\")\n",
    "    return total_to_merge\n",
    "\n",
    "\n",
    "def process_table(table_name: str) -> None:\n",
    "    if table_name not in TABLE_CONFIGS:\n",
    "        raise ValueError(f\"Tabela '{table_name}' não está configurada em TABLE_CONFIGS.\")\n",
    "\n",
    "    business_key_cols = TABLE_CONFIGS[table_name][\"business_key\"]\n",
    "\n",
    "    logger.info(f\"================ INÍCIO BRONZE: {table_name} ================\")\n",
    "\n",
    "    df_src, total_inc, new_batches = read_landing_incremental(table_name)\n",
    "\n",
    "    if df_src is None or total_inc == 0:\n",
    "        logger.info(f\"[{table_name}] Nenhum dado incremental para processar.\")\n",
    "        logger.info(f\"================ FIM BRONZE (sem dados): {table_name} ================\")\n",
    "        return\n",
    "\n",
    "    batch_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    merged = merge_into_bronze_changelog(df_src, table_name, business_key_cols, batch_id)\n",
    "\n",
    "    if merged > 0:\n",
    "        register_processed_batches(table_name, new_batches)\n",
    "\n",
    "    logger.info(\n",
    "        f\"[{table_name}] Resumo Bronze: lidos_incremental={total_inc}, \"\n",
    "        f\"mesclados_no_bronze={merged}, batch_id={batch_id}\"\n",
    "    )\n",
    "    logger.info(f\"================ FIM BRONZE: {table_name} ================\")\n",
    "\n",
    "\n",
    "def main(tables: Optional[List[str]] = None) -> None:\n",
    "    init_schema(BRONZE_SCHEMA)\n",
    "    init_meta_bronze_batches()\n",
    "\n",
    "    if tables is None:\n",
    "        tables = list(TABLE_CONFIGS.keys())\n",
    "\n",
    "    for tbl in tables:\n",
    "        process_table(tbl)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "c2b0e3fbccd5e352"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
