{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import logging\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType\n",
    "\n",
    "CATALOG = \"\"\n",
    "\n",
    "VOLUME_CATALOG = \"main\"\n",
    "VOLUME_SCHEMA = \"engenharia_dados\"\n",
    "VOLUME_NAME = \"aviacao_landing\"\n",
    "\n",
    "LANDING_CSV_BASE_PATH = f\"/Volumes/{VOLUME_CATALOG}/{VOLUME_SCHEMA}/{VOLUME_NAME}/aviacao/landing\"\n",
    "\n",
    "BRONZE_SCHEMA = \"aviacao_bronze\"\n",
    "ORIGEM_SISTEMA = \"postgres-aviacao\"\n",
    "\n",
    "LATE_ARRIVAL_LOOKBACK_SECONDS = 300\n",
    "USE_INFER_SCHEMA = True\n",
    "\n",
    "TABLE_CONFIGS: Dict[str, Dict] = {\n",
    "    \"companhias_aereas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"modelos_avioes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeroportos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeronaves\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"funcionarios\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"clientes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"voos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"reservas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bilhetes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bagagens\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"manutencoes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"tripulacao_voo\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "}\n",
    "\n",
    "TABLE_SCHEMAS: Dict[str, StructType] = {\n",
    "    # se quiser evitar inferSchema, defina aqui os schemas da landing por tabela\n",
    "    # \"companhias_aereas\": StructType([...]),\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(\"aviacao_bronze\")\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s [%(levelname)s] %(name)s - %(message)s\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def qname(schema: str, table: str) -> str:\n",
    "    if CATALOG:\n",
    "        return f\"{CATALOG}.{schema}.{table}\"\n",
    "    return f\"{schema}.{table}\"\n",
    "\n",
    "\n",
    "def now_utc():\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "\n",
    "def init_schema(schema_name: str) -> None:\n",
    "    schema_qualified = f\"{CATALOG}.{schema_name}\" if CATALOG else schema_name\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_qualified}\")\n",
    "\n",
    "\n",
    "def bronze_table_name(table_name: str) -> str:\n",
    "    return qname(BRONZE_SCHEMA, f\"{table_name}_changelog\")\n",
    "\n",
    "\n",
    "def get_landing_schema(table_name: str) -> Optional[StructType]:\n",
    "    return TABLE_SCHEMAS.get(table_name)\n",
    "\n",
    "\n",
    "def path_exists(path: str) -> bool:\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return len(files) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def read_landing_incremental(table_name: str) -> Tuple[Optional[DataFrame], int]:\n",
    "    bronze_table = bronze_table_name(table_name)\n",
    "\n",
    "    if spark.catalog.tableExists(bronze_table):\n",
    "        last_data_ref = (\n",
    "            spark.table(bronze_table)\n",
    "            .agg(F.max(\"data_ref\").alias(\"max_dr\"))\n",
    "            .collect()[0][\"max_dr\"]\n",
    "        )\n",
    "        if last_data_ref is not None:\n",
    "            margin_ts = last_data_ref - timedelta(seconds=LATE_ARRIVAL_LOOKBACK_SECONDS)\n",
    "            logger.info(\n",
    "                f\"[{table_name}] Último data_ref no Bronze: {last_data_ref} \"\n",
    "                f\"(lookback aplicado: {margin_ts})\"\n",
    "            )\n",
    "        else:\n",
    "            margin_ts = None\n",
    "    else:\n",
    "        last_data_ref = None\n",
    "        margin_ts = None\n",
    "        logger.info(f\"[{table_name}] Nenhuma tabela Bronze ainda; carga full da landing.\")\n",
    "\n",
    "    landing_path = f\"{LANDING_CSV_BASE_PATH}/{table_name}\"\n",
    "\n",
    "    if not path_exists(landing_path):\n",
    "        logger.info(f\"[{table_name}] Nenhum arquivo encontrado na landing em {landing_path}.\")\n",
    "        return None, 0\n",
    "\n",
    "    reader = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"delimiter\", \";\")\n",
    "    )\n",
    "\n",
    "    schema = get_landing_schema(table_name)\n",
    "    if schema is not None:\n",
    "        reader = reader.schema(schema)\n",
    "    elif not USE_INFER_SCHEMA:\n",
    "        raise ValueError(\n",
    "            f\"Schema fixo não configurado para {table_name} e USE_INFER_SCHEMA=False.\"\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\n",
    "            f\"[{table_name}] Usando inferSchema na landing. \"\n",
    "            f\"Configure TABLE_SCHEMAS para produção.\"\n",
    "        )\n",
    "        reader = reader.option(\"inferSchema\", \"true\")\n",
    "\n",
    "    df = reader.csv(landing_path)\n",
    "\n",
    "    if \"data_ref\" not in df.columns:\n",
    "        raise ValueError(f\"[{table_name}] Coluna data_ref não encontrada na landing.\")\n",
    "\n",
    "    df = df.withColumn(\"data_ref\", F.col(\"data_ref\").cast(\"timestamp\"))\n",
    "\n",
    "    total_raw = df.count()\n",
    "    logger.info(f\"[{table_name}] Registros brutos na landing (antes de filtros): {total_raw}\")\n",
    "\n",
    "    df_valid = df.filter(F.col(\"data_ref\").isNotNull())\n",
    "    total_valid = df_valid.count()\n",
    "    null_count = total_raw - total_valid\n",
    "    if null_count > 0:\n",
    "        logger.warning(\n",
    "            f\"[{table_name}] {null_count} registros descartados por data_ref nula.\"\n",
    "        )\n",
    "\n",
    "    if margin_ts is not None:\n",
    "        df_valid = df_valid.filter(F.col(\"data_ref\") > F.lit(margin_ts))\n",
    "\n",
    "    if \"change_op\" in df_valid.columns:\n",
    "        dist = df_valid.groupBy(\"change_op\").count().collect()\n",
    "        for row in dist:\n",
    "            logger.info(\n",
    "                f\"[{table_name}] (landing filtrada) change_op={row['change_op']} \"\n",
    "                f\"-> {row['count']} registros\"\n",
    "            )\n",
    "\n",
    "    business_key_cols = TABLE_CONFIGS[table_name][\"business_key\"]\n",
    "    df_valid = df_valid.dropDuplicates(business_key_cols + [\"data_ref\"])\n",
    "    total_final = df_valid.count()\n",
    "    logger.info(\n",
    "        f\"[{table_name}] Registros após filtros + deduplicação (landing incremental): \"\n",
    "        f\"{total_final}\"\n",
    "    )\n",
    "\n",
    "    if total_final == 0:\n",
    "        return None, 0\n",
    "\n",
    "    return df_valid, total_final\n",
    "\n",
    "\n",
    "def ensure_bronze_changelog_table(table_name: str, df_sample: DataFrame) -> None:\n",
    "    bronze_table = bronze_table_name(table_name)\n",
    "\n",
    "    if spark.catalog.tableExists(bronze_table):\n",
    "        return\n",
    "\n",
    "    logger.info(f\"[{table_name}] Criando tabela Bronze change-log vazia: {bronze_table}\")\n",
    "\n",
    "    base_schema: StructType = df_sample.schema\n",
    "    existing_cols = {f.name for f in base_schema.fields}\n",
    "\n",
    "    metadata_fields = []\n",
    "\n",
    "    if \"bronze_load_ts\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"bronze_load_ts\", TimestampType(), nullable=False)\n",
    "        )\n",
    "\n",
    "    if \"bronze_batch_id\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"bronze_batch_id\", StringType(), nullable=False)\n",
    "        )\n",
    "\n",
    "    if \"origem_sistema\" not in existing_cols:\n",
    "        metadata_fields.append(\n",
    "            StructField(\"origem_sistema\", StringType(), nullable=False)\n",
    "        )\n",
    "\n",
    "    bronze_schema = StructType(list(base_schema.fields) + metadata_fields)\n",
    "\n",
    "    empty_df = spark.createDataFrame([], bronze_schema)\n",
    "\n",
    "    (\n",
    "        empty_df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(bronze_table)\n",
    "    )\n",
    "\n",
    "    logger.info(f\"[{table_name}] Tabela Bronze criada como append-only change-log.\")\n",
    "\n",
    "\n",
    "def merge_into_bronze_changelog(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    business_key_cols: List[str],\n",
    "    batch_id: str,\n",
    ") -> int:\n",
    "    bronze_table = bronze_table_name(table_name)\n",
    "\n",
    "    ensure_bronze_changelog_table(table_name, df)\n",
    "\n",
    "    df_enriched = (\n",
    "        df\n",
    "        .withColumn(\"bronze_load_ts\", F.lit(now_utc()))\n",
    "        .withColumn(\"bronze_batch_id\", F.lit(batch_id))\n",
    "        .withColumn(\"origem_sistema\", F.lit(ORIGEM_SISTEMA).cast(\"string\"))\n",
    "    )\n",
    "\n",
    "    dedup_cols = business_key_cols + [\"data_ref\"]\n",
    "    df_enriched = df_enriched.dropDuplicates(dedup_cols)\n",
    "\n",
    "    total_to_merge = df_enriched.count()\n",
    "    logger.info(f\"[{table_name}] Registros a serem mesclados no Bronze: {total_to_merge}\")\n",
    "\n",
    "    if total_to_merge == 0:\n",
    "        return 0\n",
    "\n",
    "    if \"change_op\" in df_enriched.columns:\n",
    "        dist = df_enriched.groupBy(\"change_op\").count().collect()\n",
    "        for row in dist:\n",
    "            logger.info(\n",
    "                f\"[{table_name}] (bronze) change_op={row['change_op']} \"\n",
    "                f\"-> {row['count']} registros\"\n",
    "            )\n",
    "\n",
    "    cond_parts = [f\"tgt.{col} = src.{col}\" for col in business_key_cols]\n",
    "    cond_parts.append(\"tgt.data_ref = src.data_ref\")\n",
    "    merge_condition = \" AND \".join(cond_parts)\n",
    "\n",
    "    delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "\n",
    "    (\n",
    "        delta_tbl.alias(\"tgt\")\n",
    "        .merge(\n",
    "            df_enriched.alias(\"src\"),\n",
    "            merge_condition\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    logger.info(f\"[{table_name}] MERGE em Bronze change-log concluído.\")\n",
    "    return total_to_merge\n",
    "\n",
    "\n",
    "def process_table(table_name: str) -> None:\n",
    "    if table_name not in TABLE_CONFIGS:\n",
    "        raise ValueError(f\"Tabela '{table_name}' não está configurada em TABLE_CONFIGS.\")\n",
    "\n",
    "    business_key_cols = TABLE_CONFIGS[table_name][\"business_key\"]\n",
    "\n",
    "    logger.info(f\"================ INÍCIO BRONZE: {table_name} ================\")\n",
    "\n",
    "    df_src, total_inc = read_landing_incremental(table_name)\n",
    "\n",
    "    if df_src is None or total_inc == 0:\n",
    "        logger.info(f\"[{table_name}] Nenhum dado incremental para processar.\")\n",
    "        logger.info(f\"================ FIM BRONZE (sem dados): {table_name} ================\")\n",
    "        return\n",
    "\n",
    "    batch_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    merged = merge_into_bronze_changelog(df_src, table_name, business_key_cols, batch_id)\n",
    "\n",
    "    logger.info(\n",
    "        f\"[{table_name}] Resumo Bronze: lidos_incremental={total_inc}, \"\n",
    "        f\"mesclados_no_bronze={merged}, batch_id={batch_id}\"\n",
    "    )\n",
    "    logger.info(f\"================ FIM BRONZE: {table_name} ================\")\n",
    "\n",
    "\n",
    "def main(tables: Optional[List[str]] = None) -> None:\n",
    "    init_schema(BRONZE_SCHEMA)\n",
    "\n",
    "    if tables is None:\n",
    "        tables = list(TABLE_CONFIGS.keys())\n",
    "\n",
    "    for tbl in tables:\n",
    "        process_table(tbl)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "c2b0e3fbccd5e352"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
