{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime, timezone\n",
    "from typing import Dict\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "CATALOG = \"\"\n",
    "\n",
    "VOLUME_CATALOG = \"main\"\n",
    "VOLUME_SCHEMA = \"engenharia_dados\"\n",
    "VOLUME_NAME = \"aviacao_landing\"\n",
    "\n",
    "LANDING_CSV_BASE_PATH = f\"/Volumes/{VOLUME_CATALOG}/{VOLUME_SCHEMA}/{VOLUME_NAME}/aviacao/landing\"\n",
    "\n",
    "LANDING_SCHEMA = \"aviacao_landing\"\n",
    "META_SCHEMA = \"aviacao_meta\"\n",
    "\n",
    "JDBC_URL = \"jdbc:postgresql://aviao-metrics-databricks.c5kyywe4w1hd.us-east-1.rds.amazonaws.com:5432/postgres\"\n",
    "JDBC_USER = \"postgres\"\n",
    "JDBC_PASSWORD = \"0mZhbgsBlOtYIUW3KNXq\"\n",
    "JDBC_DRIVER = \"org.postgresql.Driver\"\n",
    "\n",
    "TABLE_CONFIGS: Dict[str, Dict] = {\n",
    "    \"companhias_aereas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"modelos_avioes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeroportos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"aeronaves\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"funcionarios\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"clientes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"voos\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"reservas\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bilhetes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"bagagens\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"manutencoes\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "    \"tripulacao_voo\": {\"schema\": \"aviacao\", \"business_key\": [\"id\"]},\n",
    "}\n",
    "\n",
    "\n",
    "def qname(schema: str, table: str) -> str:\n",
    "    if CATALOG:\n",
    "        return f\"{CATALOG}.{schema}.{table}\"\n",
    "    return f\"{schema}.{table}\"\n",
    "\n",
    "\n",
    "def now_utc() -> datetime:\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "\n",
    "def is_df_empty(df: DataFrame) -> bool:\n",
    "    return df.limit(1).count() == 0\n",
    "\n",
    "\n",
    "def init_schema(schema_name: str) -> None:\n",
    "    schema_qualified = f\"{CATALOG}.{schema_name}\" if CATALOG else schema_name\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_qualified}\")\n",
    "\n",
    "\n",
    "def init_watermark_table() -> None:\n",
    "    init_schema(META_SCHEMA)\n",
    "\n",
    "    wm_table = qname(META_SCHEMA, \"watermark_incremental\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {wm_table} (\n",
    "            tabela STRING NOT NULL,\n",
    "            ultima_data_ref TIMESTAMP NOT NULL,\n",
    "            ultima_execucao_ts TIMESTAMP NOT NULL,\n",
    "            CONSTRAINT pk_watermark_incremental PRIMARY KEY (tabela)\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def get_watermark(table_name: str) -> datetime:\n",
    "    wm_table = qname(META_SCHEMA, \"watermark_incremental\")\n",
    "    df = spark.table(wm_table).filter(F.col(\"tabela\") == table_name)\n",
    "\n",
    "    if is_df_empty(df):\n",
    "        return datetime(1900, 1, 1, tzinfo=timezone.utc)\n",
    "\n",
    "    row = df.select(\"ultima_data_ref\").head(1)[0]\n",
    "    return row[\"ultima_data_ref\"]\n",
    "\n",
    "\n",
    "def update_watermark(table_name: str, new_data_ref: datetime) -> None:\n",
    "    wm_table = qname(META_SCHEMA, \"watermark_incremental\")\n",
    "    ts_exec = now_utc()\n",
    "\n",
    "    (\n",
    "        spark.table(wm_table)\n",
    "        .filter(F.col(\"tabela\") != table_name)\n",
    "        .createOrReplaceTempView(\"tmp_wm_others\")\n",
    "    )\n",
    "\n",
    "    others = spark.table(\"tmp_wm_others\")\n",
    "\n",
    "    new_row = spark.createDataFrame(\n",
    "        [(table_name, new_data_ref, ts_exec)],\n",
    "        [\"tabela\", \"ultima_data_ref\", \"ultima_execucao_ts\"],\n",
    "    )\n",
    "\n",
    "    merged = others.unionByName(new_row, allowMissingColumns=True)\n",
    "    merged.write.mode(\"overwrite\").format(\"delta\").saveAsTable(wm_table)\n",
    "\n",
    "\n",
    "def build_incremental_query(pg_schema: str, table_name: str, last_wm: datetime) -> str:\n",
    "    last_wm_str = last_wm.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return f\"\"\"\n",
    "        SELECT\n",
    "            t.*,\n",
    "            COALESCE(t.AUD_DH_ALTERACAO, t.AUD_DH_CRIACAO) AS data_ref,\n",
    "            CASE\n",
    "                WHEN t.AUD_DH_ALTERACAO IS NULL THEN 'I'\n",
    "                ELSE 'U'\n",
    "            END AS change_op\n",
    "        FROM {pg_schema}.{table_name} t\n",
    "        WHERE COALESCE(t.AUD_DH_ALTERACAO, t.AUD_DH_CRIACAO) > TIMESTAMP '{last_wm_str}'\n",
    "        ORDER BY data_ref, id\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def extract_incremental_from_postgres(table_name: str, conf: Dict, last_wm: datetime) -> DataFrame:\n",
    "    pg_schema = conf[\"schema\"]\n",
    "    query = build_incremental_query(pg_schema, table_name, last_wm)\n",
    "\n",
    "    print(f\"[{table_name}] Executando query incremental a partir de {last_wm} ...\")\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", JDBC_URL)\n",
    "        .option(\"driver\", JDBC_DRIVER)\n",
    "        .option(\"user\", JDBC_USER)\n",
    "        .option(\"password\", JDBC_PASSWORD)\n",
    "        .option(\"dbtable\", f\"({query}) AS src\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    total = df.count()\n",
    "    print(f\"[{table_name}] Registros incrementais lidos do Postgres: {total}\")\n",
    "\n",
    "    if total > 0 and \"change_op\" in df.columns:\n",
    "        dist = df.groupBy(\"change_op\").count().collect()\n",
    "        for row in dist:\n",
    "            print(f\"[{table_name}] change_op={row['change_op']} -> {row['count']} registros\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_landing_csv(df: DataFrame, table_name: str, batch_id: str) -> str:\n",
    "    landing_path = f\"{LANDING_CSV_BASE_PATH}/{table_name}/batch_id={batch_id}\"\n",
    "\n",
    "    df_to_write = (\n",
    "        df\n",
    "        .withColumn(\"landing_batch_id\", F.lit(batch_id))\n",
    "        .withColumn(\"landing_load_ts\", F.lit(now_utc()))\n",
    "    )\n",
    "\n",
    "    total_to_write = df_to_write.count()\n",
    "    print(f\"[{table_name}] Registros a serem gravados em CSV: {total_to_write}\")\n",
    "\n",
    "    if total_to_write > 0 and \"change_op\" in df_to_write.columns:\n",
    "        dist = df_to_write.groupBy(\"change_op\").count().collect()\n",
    "        for row in dist:\n",
    "            print(f\"[{table_name}] (CSV) change_op={row['change_op']} -> {row['count']} registros\")\n",
    "\n",
    "    (\n",
    "        df_to_write.write\n",
    "        .mode(\"append\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"delimiter\", \";\")\n",
    "        .csv(landing_path)\n",
    "    )\n",
    "\n",
    "    print(f\"[{table_name}] Landing CSV gravada em: {landing_path}\")\n",
    "    return landing_path\n",
    "\n",
    "\n",
    "def run_landing_for_table(table_name: str) -> None:\n",
    "    if table_name not in TABLE_CONFIGS:\n",
    "        raise ValueError(f\"Tabela '{table_name}' não está configurada em TABLE_CONFIGS.\")\n",
    "\n",
    "    conf = TABLE_CONFIGS[table_name]\n",
    "\n",
    "    print(f\"================ INÍCIO LANDING: {table_name} ================\")\n",
    "\n",
    "    last_wm = get_watermark(table_name)\n",
    "    print(f\"[{table_name}] Watermark atual: {last_wm}\")\n",
    "\n",
    "    df_src = extract_incremental_from_postgres(table_name, conf, last_wm)\n",
    "\n",
    "    if is_df_empty(df_src):\n",
    "        print(f\"[{table_name}] Nenhuma linha nova/atualizada desde o último watermark.\")\n",
    "        update_watermark(table_name, last_wm)\n",
    "        print(f\"================ FIM LANDING (sem dados): {table_name} ================\")\n",
    "        return\n",
    "\n",
    "    batch_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    write_landing_csv(df_src, table_name, batch_id)\n",
    "\n",
    "    new_max_data_ref = df_src.agg(F.max(\"data_ref\").alias(\"max_dr\")).collect()[0][\"max_dr\"]\n",
    "    update_watermark(table_name, new_max_data_ref)\n",
    "\n",
    "    print(f\"[{table_name}] Novo watermark: {new_max_data_ref}\")\n",
    "    print(f\"================ FIM LANDING: {table_name} ================\")\n",
    "\n",
    "\n",
    "init_schema(META_SCHEMA)\n",
    "init_watermark_table()\n",
    "\n",
    "for tbl in TABLE_CONFIGS.keys():\n",
    "    run_landing_for_table(tbl)\n"
   ],
   "id": "e14d06a3e696691b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
